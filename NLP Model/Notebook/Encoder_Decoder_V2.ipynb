{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>ASL Gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love to cook hamburgers on the grill.</td>\n",
       "      <td>G-R-I-L-L HAMBURGER COOK I LOVE.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like to ice skate on our pond.</td>\n",
       "      <td>OUR P-O-N-D ICE SKATE I LIKE.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I like to be active and not sit and watch TV a...</td>\n",
       "      <td>ALL DAY I LIKE ACTIVE NOT SIT WATCH #TV.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are you prepared for hurricane season?</td>\n",
       "      <td>HURRICANE S-E-A-S-O-N YOU READY PREPARE?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The dinner party was awkward because most of u...</td>\n",
       "      <td>DINNER PARTY AWKWARD WHY? MOST US NOT KNOW EA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0            I love to cook hamburgers on the grill.   \n",
       "1                   I like to ice skate on our pond.   \n",
       "2  I like to be active and not sit and watch TV a...   \n",
       "3             Are you prepared for hurricane season?   \n",
       "4  The dinner party was awkward because most of u...   \n",
       "\n",
       "                                           ASL Gloss  \n",
       "0                   G-R-I-L-L HAMBURGER COOK I LOVE.  \n",
       "1                      OUR P-O-N-D ICE SKATE I LIKE.  \n",
       "2           ALL DAY I LIKE ACTIVE NOT SIT WATCH #TV.  \n",
       "3           HURRICANE S-E-A-S-O-N YOU READY PREPARE?  \n",
       "4   DINNER PARTY AWKWARD WHY? MOST US NOT KNOW EA...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('ASL_English.csv')\n",
    "df = pd.read_csv('..\\Data\\Dataset_EnglishToGloss.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1428, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Note:\n",
    "- Replace the numbers/digits\n",
    "- Check regarding Finger spellings\n",
    "- Check if it is required to add start and end tokens to target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {'1': \" one \", '2':\" two \", '3':\" three \", '4':\" four \", '5':\" five \", '6':\" six \", '7':\" seven \", '8':\" eight \", '9':\" nine \", '0':\" zero \"}\n",
    "df['English'] = df['English'].apply(lambda x: re.sub('(\\d)', lambda m: replacements[m.group()], x))\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: re.sub('(\\d)', lambda m: replacements[m.group()], x))\n",
    "\n",
    "# Remove extra spaces\n",
    "df['English'] = df['English'].apply(lambda x: x.strip())\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: x.strip())\n",
    "\n",
    "# Lowercase all characters\n",
    "df['English'] = df['English'].apply(lambda x: x.lower())\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply (lambda x: x.lower())\n",
    "\n",
    "# Removing double spaces\n",
    "df['English'] = df['English'].apply(lambda x: x.replace('  ', ' '))\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: x.replace('  ', ' '))\n",
    "\n",
    "# Remove quotes # Might not need this\n",
    "# df['English'] = df['English'].apply(lambda x: re.sub (r\"'\", '', x))\n",
    "# df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: re.sub (r\"'\", '', x))\n",
    "\n",
    "# Remove all special character\n",
    "df['English'] = df['English'].apply(lambda x: ''.join (ch for ch in x if ch not in set(string.punctuation)))\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: ''.join (ch for ch in x if ch not in set(string.punctuation)))\n",
    "\n",
    "# Check if dataset has numbers\n",
    "#print(df['English'].str.contains(r'\\d').any())\n",
    "#print(df['ASL Gloss'].str.contains(r'\\d').any())\n",
    "\n",
    "# Add tokens to target sequence\n",
    "df['English'] = df['English'].apply(lambda x : 'START_ ' + x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>ASL Gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>START_ i love to cook hamburgers on the grill ...</td>\n",
       "      <td>grill hamburger cook i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>START_ i like to ice skate on our pond _END</td>\n",
       "      <td>our pond ice skate i like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>START_ i like to be active and not sit and wat...</td>\n",
       "      <td>all day i like active not sit watch tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>START_ are you prepared for hurricane season _END</td>\n",
       "      <td>hurricane season you ready prepare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>START_ the dinner party was awkward because mo...</td>\n",
       "      <td>dinner party awkward why most us not know each...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>START_ my favorite author is me _END</td>\n",
       "      <td>my favorite author who me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>START_ i love anything that has chocolate in i...</td>\n",
       "      <td>i love anything have chocolate inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>START_ in one nine four seven rhulin thomas be...</td>\n",
       "      <td>year one nine four seven rhulin thomas he firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>START_ sean berdy is a deaf actor well known f...</td>\n",
       "      <td>sean berdy himself deaf actor famous why he in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>START_ did you see the movie about the life of...</td>\n",
       "      <td>you finish see movie about deaf woman herself ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  START_ i love to cook hamburgers on the grill ...   \n",
       "1        START_ i like to ice skate on our pond _END   \n",
       "2  START_ i like to be active and not sit and wat...   \n",
       "3  START_ are you prepared for hurricane season _END   \n",
       "4  START_ the dinner party was awkward because mo...   \n",
       "5               START_ my favorite author is me _END   \n",
       "6  START_ i love anything that has chocolate in i...   \n",
       "7  START_ in one nine four seven rhulin thomas be...   \n",
       "8  START_ sean berdy is a deaf actor well known f...   \n",
       "9  START_ did you see the movie about the life of...   \n",
       "\n",
       "                                           ASL Gloss  \n",
       "0                        grill hamburger cook i love  \n",
       "1                          our pond ice skate i like  \n",
       "2             all day i like active not sit watch tv  \n",
       "3                 hurricane season you ready prepare  \n",
       "4  dinner party awkward why most us not know each...  \n",
       "5                          my favorite author who me  \n",
       "6              i love anything have chocolate inside  \n",
       "7  year one nine four seven rhulin thomas he firs...  \n",
       "8  sean berdy himself deaf actor famous why he in...  \n",
       "9  you finish see movie about deaf woman herself ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>ASL Gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>START_ the cow is brown _END</td>\n",
       "      <td>cow color what brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>START_ the deer is tan _END</td>\n",
       "      <td>deer color what tan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>START_ the horse is white _END</td>\n",
       "      <td>horse color what white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>START_ the cat is black _END</td>\n",
       "      <td>cat color what black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>START_ the mouse is gray _END</td>\n",
       "      <td>mouse color what gray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>START_ the lion is gold _END</td>\n",
       "      <td>lion color what gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>START_ the snake is silver _END</td>\n",
       "      <td>snake color what silver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>START_ i love whales _END</td>\n",
       "      <td>whales i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>START_ i like frogs _END</td>\n",
       "      <td>frogs i like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>START_ juneteenth is tuesday june one nine _END</td>\n",
       "      <td>juneteenth when tuesday june one nine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              English  \\\n",
       "1418                     START_ the cow is brown _END   \n",
       "1419                      START_ the deer is tan _END   \n",
       "1420                   START_ the horse is white _END   \n",
       "1421                     START_ the cat is black _END   \n",
       "1422                    START_ the mouse is gray _END   \n",
       "1423                     START_ the lion is gold _END   \n",
       "1424                  START_ the snake is silver _END   \n",
       "1425                        START_ i love whales _END   \n",
       "1426                         START_ i like frogs _END   \n",
       "1427  START_ juneteenth is tuesday june one nine _END   \n",
       "\n",
       "                                  ASL Gloss  \n",
       "1418                   cow color what brown  \n",
       "1419                    deer color what tan  \n",
       "1420                 horse color what white  \n",
       "1421                   cat color what black  \n",
       "1422                  mouse color what gray  \n",
       "1423                   lion color what gold  \n",
       "1424                snake color what silver  \n",
       "1425                          whales i love  \n",
       "1426                           frogs i like  \n",
       "1427  juneteenth when tuesday june one nine  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ASL Gloss'].str.len().sort_values(ascending=False).head()\n",
    "# df['English'].str.len().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_eng_words = set()\n",
    "for eng in df['English']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of ASL \n",
    "all_ASL_words = set()\n",
    "for asl in df['ASL Gloss']:\n",
    "    for word in asl.split():\n",
    "        if word not in all_ASL_words:\n",
    "            all_ASL_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length target:  72\n"
     ]
    }
   ],
   "source": [
    "# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in df ['English']:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "print(\"Max length target: \", max_length_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length sorce:  40\n"
     ]
    }
   ],
   "source": [
    "# Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in df ['ASL Gloss']:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "print(\"Max length sorce: \", max_length_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 2033)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_ASL_words))\n",
    "target_words = sorted(list(all_eng_words))\n",
    "\n",
    "# Calculate Vocab size for both source and target\n",
    "num_encoder_tokens = len(all_ASL_words) + 1\n",
    "num_decoder_tokens = len(all_eng_words) + 1\n",
    "\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word to token dictionary for both source and target\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "# Create token to word dictionary for both source and target\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>ASL Gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>START_ is your teacher deaf or hearing _END</td>\n",
       "      <td>your teacher deaf hearing which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>START_ i want soup and salad for lunch _END</td>\n",
       "      <td>lunch soup salad i want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>START_ i watched a boring movie last night and...</td>\n",
       "      <td>last night boring movie i watch fallasleep me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>START_ i love watching sports every saturday _END</td>\n",
       "      <td>every saturday sports watch i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>START_ my parents are members of the indianapo...</td>\n",
       "      <td>indianapolis deaf club my parents members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>START_ the golf game is at five zero zero pm _END</td>\n",
       "      <td>time five zero zero pm golf game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>START_ i have seven shirts _END</td>\n",
       "      <td>seven shirts i have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>START_ im not hungry ill just order soup _END</td>\n",
       "      <td>me not hungry soup order finish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>START_ there was a famous protest at gallaudet...</td>\n",
       "      <td>back year one nine eight eight gallaudet unive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>START_ i like trees i love pine trees _END</td>\n",
       "      <td>i like trees i love pine trees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                English  \\\n",
       "519         START_ is your teacher deaf or hearing _END   \n",
       "220         START_ i want soup and salad for lunch _END   \n",
       "15    START_ i watched a boring movie last night and...   \n",
       "59    START_ i love watching sports every saturday _END   \n",
       "726   START_ my parents are members of the indianapo...   \n",
       "1296  START_ the golf game is at five zero zero pm _END   \n",
       "1401                    START_ i have seven shirts _END   \n",
       "613       START_ im not hungry ill just order soup _END   \n",
       "454   START_ there was a famous protest at gallaudet...   \n",
       "1218         START_ i like trees i love pine trees _END   \n",
       "\n",
       "                                              ASL Gloss  \n",
       "519                     your teacher deaf hearing which  \n",
       "220                             lunch soup salad i want  \n",
       "15        last night boring movie i watch fallasleep me  \n",
       "59                   every saturday sports watch i love  \n",
       "726           indianapolis deaf club my parents members  \n",
       "1296                   time five zero zero pm golf game  \n",
       "1401                                seven shirts i have  \n",
       "613                     me not hungry soup order finish  \n",
       "454   back year one nine eight eight gallaudet unive...  \n",
       "1218                     i like trees i love pine trees  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(df).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a 90â€“10 train and test split and write a Python generator function to load the data in batches as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1285,), (143,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train - Test Split\n",
    "X, y = df['ASL Gloss'], df['English']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the train and test dataframes for reproducing the results later, as they are shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('Weights_ASL/X_train.pkl')\n",
    "X_test.to_pickle('Weights_ASL/X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X.iloc[j:j+batch_size], y.iloc[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder - Decoder Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# Use a softmax to generate a probability distribution over the target vocabulary for each time step\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network for 50 epochs with a batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "# epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10/10 [==============================] - 9s 894ms/step - loss: 4.3326 - acc: 0.1894 - val_loss: 4.9394 - val_acc: 0.1876\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 8s 826ms/step - loss: 4.3111 - acc: 0.1899 - val_loss: 4.9242 - val_acc: 0.1886\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 4.2902 - acc: 0.1909 - val_loss: 4.9021 - val_acc: 0.1895\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 10s 945ms/step - loss: 4.2690 - acc: 0.1923 - val_loss: 4.8808 - val_acc: 0.1905\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 12s 1s/step - loss: 4.2478 - acc: 0.1931 - val_loss: 4.8623 - val_acc: 0.1905\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 9s 907ms/step - loss: 4.2271 - acc: 0.1945 - val_loss: 4.8407 - val_acc: 0.1924\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 10s 979ms/step - loss: 4.2062 - acc: 0.1954 - val_loss: 4.8247 - val_acc: 0.1933\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 4.1864 - acc: 0.1973 - val_loss: 4.8012 - val_acc: 0.1952\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 8s 793ms/step - loss: 4.1660 - acc: 0.1991 - val_loss: 4.7824 - val_acc: 0.1952\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 4.1460 - acc: 0.2000 - val_loss: 4.7625 - val_acc: 0.1971\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 8s 841ms/step - loss: 4.1266 - acc: 0.2021 - val_loss: 4.7438 - val_acc: 0.1990\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 9s 932ms/step - loss: 4.1068 - acc: 0.2036 - val_loss: 4.7229 - val_acc: 0.1990\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 7s 729ms/step - loss: 4.0876 - acc: 0.2038 - val_loss: 4.7029 - val_acc: 0.2000\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 8s 783ms/step - loss: 4.0676 - acc: 0.2039 - val_loss: 4.6833 - val_acc: 0.1990\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 7s 739ms/step - loss: 4.0490 - acc: 0.2036 - val_loss: 4.6622 - val_acc: 0.1981\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 7s 721ms/step - loss: 4.0293 - acc: 0.2039 - val_loss: 4.6419 - val_acc: 0.2000\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 9s 877ms/step - loss: 4.0106 - acc: 0.2051 - val_loss: 4.6255 - val_acc: 0.2010\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 9s 936ms/step - loss: 3.9924 - acc: 0.2060 - val_loss: 4.6046 - val_acc: 0.2029\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 3.9740 - acc: 0.2074 - val_loss: 4.5870 - val_acc: 0.2019\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 9s 844ms/step - loss: 3.9569 - acc: 0.2075 - val_loss: 4.5674 - val_acc: 0.2029\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 8s 790ms/step - loss: 3.9380 - acc: 0.2084 - val_loss: 4.5474 - val_acc: 0.2048\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 9s 859ms/step - loss: 3.9202 - acc: 0.2099 - val_loss: 4.5308 - val_acc: 0.2057\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 9s 958ms/step - loss: 3.9039 - acc: 0.2105 - val_loss: 4.5095 - val_acc: 0.2086\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 9s 894ms/step - loss: 3.8843 - acc: 0.2125 - val_loss: 4.4865 - val_acc: 0.2076\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 7s 725ms/step - loss: 3.8641 - acc: 0.2130 - val_loss: 4.4660 - val_acc: 0.2114\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 7s 715ms/step - loss: 3.8454 - acc: 0.2173 - val_loss: 4.4477 - val_acc: 0.2095\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 10s 974ms/step - loss: 3.8270 - acc: 0.2201 - val_loss: 4.4283 - val_acc: 0.2114\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 9s 871ms/step - loss: 3.8096 - acc: 0.2221 - val_loss: 4.4047 - val_acc: 0.2190\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 3.7875 - acc: 0.2259 - val_loss: 4.3804 - val_acc: 0.2248\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 9s 880ms/step - loss: 3.7679 - acc: 0.2300 - val_loss: 4.3581 - val_acc: 0.2257\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 10s 987ms/step - loss: 3.7474 - acc: 0.2303 - val_loss: 4.3279 - val_acc: 0.2248\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 9s 898ms/step - loss: 3.7214 - acc: 0.2324 - val_loss: 4.3081 - val_acc: 0.2276\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 9s 945ms/step - loss: 3.7011 - acc: 0.2331 - val_loss: 4.2812 - val_acc: 0.2276\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 9s 911ms/step - loss: 3.6761 - acc: 0.2346 - val_loss: 4.2597 - val_acc: 0.2295\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 8s 760ms/step - loss: 3.6554 - acc: 0.2339 - val_loss: 4.2337 - val_acc: 0.2295\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 7s 669ms/step - loss: 3.6302 - acc: 0.2346 - val_loss: 4.2119 - val_acc: 0.2314\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 8s 808ms/step - loss: 3.6115 - acc: 0.2363 - val_loss: 4.1865 - val_acc: 0.2314\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 3.5842 - acc: 0.2367 - val_loss: 4.1684 - val_acc: 0.2343\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 3.5634 - acc: 0.2394 - val_loss: 4.1359 - val_acc: 0.2362\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 9s 882ms/step - loss: 3.5389 - acc: 0.2404 - val_loss: 4.1128 - val_acc: 0.2390\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 7s 743ms/step - loss: 3.5141 - acc: 0.2430 - val_loss: 4.0903 - val_acc: 0.2362\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 3.4920 - acc: 0.2434 - val_loss: 4.0613 - val_acc: 0.2410\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 12s 1s/step - loss: 3.4679 - acc: 0.2464 - val_loss: 4.0472 - val_acc: 0.2457\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 9s 932ms/step - loss: 3.4435 - acc: 0.2507 - val_loss: 4.0161 - val_acc: 0.2467\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 9s 897ms/step - loss: 3.4203 - acc: 0.2524 - val_loss: 3.9878 - val_acc: 0.2486\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 3.3957 - acc: 0.2536 - val_loss: 3.9668 - val_acc: 0.2543\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 9s 857ms/step - loss: 3.3735 - acc: 0.2560 - val_loss: 3.9394 - val_acc: 0.2543\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 8s 823ms/step - loss: 3.3500 - acc: 0.2571 - val_loss: 3.9206 - val_acc: 0.2524\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 14s 1s/step - loss: 3.3272 - acc: 0.2584 - val_loss: 3.8959 - val_acc: 0.2552\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 9s 881ms/step - loss: 3.3036 - acc: 0.2608 - val_loss: 3.8701 - val_acc: 0.2600\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 14s 1s/step - loss: 3.2806 - acc: 0.2632 - val_loss: 3.8504 - val_acc: 0.2629\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 10s 971ms/step - loss: 3.2576 - acc: 0.2656 - val_loss: 3.8278 - val_acc: 0.2638\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 9s 919ms/step - loss: 3.2354 - acc: 0.2682 - val_loss: 3.8019 - val_acc: 0.2619\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 9s 901ms/step - loss: 3.2114 - acc: 0.2686 - val_loss: 3.7849 - val_acc: 0.2686\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 8s 754ms/step - loss: 3.1911 - acc: 0.2711 - val_loss: 3.7545 - val_acc: 0.2705\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 9s 888ms/step - loss: 3.1664 - acc: 0.2754 - val_loss: 3.7398 - val_acc: 0.2705\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 9s 925ms/step - loss: 3.1476 - acc: 0.2764 - val_loss: 3.7121 - val_acc: 0.2743\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 9s 958ms/step - loss: 3.1211 - acc: 0.2836 - val_loss: 3.6931 - val_acc: 0.2762\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 3.1031 - acc: 0.2836 - val_loss: 3.6734 - val_acc: 0.2771\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 3.0787 - acc: 0.2862 - val_loss: 3.6441 - val_acc: 0.2810\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 3.0575 - acc: 0.2891 - val_loss: 3.6271 - val_acc: 0.2819\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 9s 857ms/step - loss: 3.0336 - acc: 0.2917 - val_loss: 3.6011 - val_acc: 0.2848\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 8s 825ms/step - loss: 3.0110 - acc: 0.2958 - val_loss: 3.5807 - val_acc: 0.2905\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 8s 821ms/step - loss: 2.9912 - acc: 0.2980 - val_loss: 3.5633 - val_acc: 0.2895\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 8s 846ms/step - loss: 2.9674 - acc: 0.2996 - val_loss: 3.5398 - val_acc: 0.2886\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 9s 871ms/step - loss: 2.9507 - acc: 0.3009 - val_loss: 3.5146 - val_acc: 0.2962\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 9s 906ms/step - loss: 2.9217 - acc: 0.3071 - val_loss: 3.4936 - val_acc: 0.2971\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 10s 976ms/step - loss: 2.9061 - acc: 0.3097 - val_loss: 3.4788 - val_acc: 0.2990\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 8s 819ms/step - loss: 2.8821 - acc: 0.3109 - val_loss: 3.4570 - val_acc: 0.2981\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 10s 931ms/step - loss: 2.8635 - acc: 0.3143 - val_loss: 3.4277 - val_acc: 0.3019\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 8s 868ms/step - loss: 2.8430 - acc: 0.3157 - val_loss: 3.4112 - val_acc: 0.3038\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 8s 780ms/step - loss: 2.8210 - acc: 0.3195 - val_loss: 3.3986 - val_acc: 0.3057\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 7s 711ms/step - loss: 2.8071 - acc: 0.3191 - val_loss: 3.3671 - val_acc: 0.3076\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 7s 719ms/step - loss: 2.7799 - acc: 0.3231 - val_loss: 3.3567 - val_acc: 0.3095\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 8s 781ms/step - loss: 2.7612 - acc: 0.3253 - val_loss: 3.3293 - val_acc: 0.3067\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 8s 852ms/step - loss: 2.7417 - acc: 0.3278 - val_loss: 3.3069 - val_acc: 0.3114\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 10s 982ms/step - loss: 2.7222 - acc: 0.3299 - val_loss: 3.2812 - val_acc: 0.3124\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 8s 836ms/step - loss: 2.7011 - acc: 0.3325 - val_loss: 3.2837 - val_acc: 0.3124\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 9s 846ms/step - loss: 2.6822 - acc: 0.3344 - val_loss: 3.2403 - val_acc: 0.3171\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 8s 815ms/step - loss: 2.6588 - acc: 0.3373 - val_loss: 3.2296 - val_acc: 0.3181\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 8s 767ms/step - loss: 2.6394 - acc: 0.3397 - val_loss: 3.2063 - val_acc: 0.3229\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 7s 739ms/step - loss: 2.6173 - acc: 0.3412 - val_loss: 3.1891 - val_acc: 0.3219\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 7s 691ms/step - loss: 2.6051 - acc: 0.3421 - val_loss: 3.1598 - val_acc: 0.3276\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 7s 703ms/step - loss: 2.5784 - acc: 0.3457 - val_loss: 3.1457 - val_acc: 0.3238\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 9s 962ms/step - loss: 2.5647 - acc: 0.3428 - val_loss: 3.1166 - val_acc: 0.3333\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 10s 992ms/step - loss: 2.5373 - acc: 0.3491 - val_loss: 3.1027 - val_acc: 0.3248\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 2.5205 - acc: 0.3494 - val_loss: 3.0765 - val_acc: 0.3390\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 9s 860ms/step - loss: 2.4988 - acc: 0.3549 - val_loss: 3.0712 - val_acc: 0.3324\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 8s 778ms/step - loss: 2.4791 - acc: 0.3572 - val_loss: 3.0448 - val_acc: 0.3362\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 8s 761ms/step - loss: 2.4669 - acc: 0.3575 - val_loss: 3.0224 - val_acc: 0.3410\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 8s 827ms/step - loss: 2.4371 - acc: 0.3628 - val_loss: 2.9968 - val_acc: 0.3410\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 10s 984ms/step - loss: 2.4166 - acc: 0.3645 - val_loss: 2.9757 - val_acc: 0.3457\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 8s 840ms/step - loss: 2.4037 - acc: 0.3673 - val_loss: 2.9660 - val_acc: 0.3448\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 9s 766ms/step - loss: 2.3790 - acc: 0.3714 - val_loss: 2.9338 - val_acc: 0.3505\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 7s 703ms/step - loss: 2.3579 - acc: 0.3762 - val_loss: 2.9328 - val_acc: 0.3552\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 7s 682ms/step - loss: 2.3457 - acc: 0.3793 - val_loss: 2.9263 - val_acc: 0.3514\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 7s 767ms/step - loss: 2.3220 - acc: 0.3813 - val_loss: 2.8751 - val_acc: 0.3610\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 7s 710ms/step - loss: 2.2991 - acc: 0.3880 - val_loss: 2.8674 - val_acc: 0.3638\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 9s 955ms/step - loss: 2.2895 - acc: 0.3871 - val_loss: 2.8346 - val_acc: 0.3676\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 7s 734ms/step - loss: 2.2563 - acc: 0.3935 - val_loss: 2.8165 - val_acc: 0.3724\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 2.2452 - acc: 0.3967 - val_loss: 2.7902 - val_acc: 0.3771\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 10s 902ms/step - loss: 2.2164 - acc: 0.4021 - val_loss: 2.7777 - val_acc: 0.3810\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 8s 837ms/step - loss: 2.2112 - acc: 0.4026 - val_loss: 2.7514 - val_acc: 0.3829\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 8s 829ms/step - loss: 2.1783 - acc: 0.4075 - val_loss: 2.7271 - val_acc: 0.3886\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 9s 918ms/step - loss: 2.1630 - acc: 0.4122 - val_loss: 2.7313 - val_acc: 0.3895\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 9s 927ms/step - loss: 2.1469 - acc: 0.4142 - val_loss: 2.6884 - val_acc: 0.3971\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 10s 945ms/step - loss: 2.1228 - acc: 0.4197 - val_loss: 2.6723 - val_acc: 0.3924\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 9s 922ms/step - loss: 2.1034 - acc: 0.4223 - val_loss: 2.6477 - val_acc: 0.4038\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 9s 947ms/step - loss: 2.0900 - acc: 0.4238 - val_loss: 2.6449 - val_acc: 0.4000\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 9s 914ms/step - loss: 2.0670 - acc: 0.4259 - val_loss: 2.6133 - val_acc: 0.4067\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 10s 963ms/step - loss: 2.0441 - acc: 0.4290 - val_loss: 2.5889 - val_acc: 0.4105\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 8s 833ms/step - loss: 2.0308 - acc: 0.4310 - val_loss: 2.5766 - val_acc: 0.4086\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 8s 843ms/step - loss: 2.0115 - acc: 0.4341 - val_loss: 2.5520 - val_acc: 0.4133\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 1.9937 - acc: 0.4374 - val_loss: 2.5429 - val_acc: 0.4162\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 9s 898ms/step - loss: 1.9747 - acc: 0.4428 - val_loss: 2.5134 - val_acc: 0.4219\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 9s 930ms/step - loss: 1.9548 - acc: 0.4463 - val_loss: 2.5030 - val_acc: 0.4219\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 9s 908ms/step - loss: 1.9397 - acc: 0.4494 - val_loss: 2.4782 - val_acc: 0.4286\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 8s 807ms/step - loss: 1.9216 - acc: 0.4530 - val_loss: 2.4668 - val_acc: 0.4276\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 9s 862ms/step - loss: 1.9048 - acc: 0.4567 - val_loss: 2.4422 - val_acc: 0.4371\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 9s 886ms/step - loss: 1.8882 - acc: 0.4605 - val_loss: 2.4277 - val_acc: 0.4381\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.8694 - acc: 0.4665 - val_loss: 2.4061 - val_acc: 0.4429\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 8s 837ms/step - loss: 1.8531 - acc: 0.4686 - val_loss: 2.3923 - val_acc: 0.4467\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 8s 829ms/step - loss: 1.8392 - acc: 0.4722 - val_loss: 2.3881 - val_acc: 0.4429\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 9s 925ms/step - loss: 1.8209 - acc: 0.4751 - val_loss: 2.3520 - val_acc: 0.4505\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 7s 748ms/step - loss: 1.8011 - acc: 0.4800 - val_loss: 2.3426 - val_acc: 0.4571\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 7s 718ms/step - loss: 1.7871 - acc: 0.4828 - val_loss: 2.3177 - val_acc: 0.4600\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 8s 795ms/step - loss: 1.7686 - acc: 0.4874 - val_loss: 2.3055 - val_acc: 0.4600\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 7s 748ms/step - loss: 1.7502 - acc: 0.4896 - val_loss: 2.2846 - val_acc: 0.4657\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 9s 946ms/step - loss: 1.7346 - acc: 0.4932 - val_loss: 2.2702 - val_acc: 0.4657\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 10s 983ms/step - loss: 1.7153 - acc: 0.4970 - val_loss: 2.2490 - val_acc: 0.4676\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 8s 840ms/step - loss: 1.6993 - acc: 0.5018 - val_loss: 2.2306 - val_acc: 0.4743\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.6801 - acc: 0.5078 - val_loss: 2.2222 - val_acc: 0.4790\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 9s 954ms/step - loss: 1.6641 - acc: 0.5136 - val_loss: 2.1950 - val_acc: 0.4838\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 10s 986ms/step - loss: 1.6474 - acc: 0.5176 - val_loss: 2.1828 - val_acc: 0.4924\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 9s 958ms/step - loss: 1.6310 - acc: 0.5224 - val_loss: 2.1663 - val_acc: 0.4933\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 9s 893ms/step - loss: 1.6141 - acc: 0.5280 - val_loss: 2.1517 - val_acc: 0.4990\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 10s 974ms/step - loss: 1.6011 - acc: 0.5318 - val_loss: 2.1266 - val_acc: 0.4990\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 1.5827 - acc: 0.5366 - val_loss: 2.1225 - val_acc: 0.5105\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 9s 854ms/step - loss: 1.5680 - acc: 0.5431 - val_loss: 2.0993 - val_acc: 0.5086\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 9s 885ms/step - loss: 1.5534 - acc: 0.5460 - val_loss: 2.0860 - val_acc: 0.5171\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 9s 855ms/step - loss: 1.5374 - acc: 0.5517 - val_loss: 2.0667 - val_acc: 0.5143\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 9s 904ms/step - loss: 1.5229 - acc: 0.5520 - val_loss: 2.0519 - val_acc: 0.5257\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 1.5083 - acc: 0.5577 - val_loss: 2.0328 - val_acc: 0.5200\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 9s 896ms/step - loss: 1.4939 - acc: 0.5597 - val_loss: 2.0178 - val_acc: 0.5276\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 9s 884ms/step - loss: 1.4772 - acc: 0.5652 - val_loss: 2.0018 - val_acc: 0.5305\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 12s 1s/step - loss: 1.4642 - acc: 0.5674 - val_loss: 1.9861 - val_acc: 0.5352\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 9s 854ms/step - loss: 1.4475 - acc: 0.5745 - val_loss: 1.9724 - val_acc: 0.5381\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 9s 919ms/step - loss: 1.4355 - acc: 0.5758 - val_loss: 1.9565 - val_acc: 0.5486\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 8s 827ms/step - loss: 1.4195 - acc: 0.5817 - val_loss: 1.9375 - val_acc: 0.5467\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 9s 866ms/step - loss: 1.4065 - acc: 0.5827 - val_loss: 1.9217 - val_acc: 0.5524\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 9s 957ms/step - loss: 1.3911 - acc: 0.5866 - val_loss: 1.9053 - val_acc: 0.5543\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 9s 920ms/step - loss: 1.3778 - acc: 0.5897 - val_loss: 1.8932 - val_acc: 0.5562\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 10s 925ms/step - loss: 1.3629 - acc: 0.5933 - val_loss: 1.8748 - val_acc: 0.5610\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 9s 857ms/step - loss: 1.3492 - acc: 0.5962 - val_loss: 1.8650 - val_acc: 0.5600\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 8s 838ms/step - loss: 1.3373 - acc: 0.5966 - val_loss: 1.8455 - val_acc: 0.5610\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 8s 756ms/step - loss: 1.3208 - acc: 0.5998 - val_loss: 1.8339 - val_acc: 0.5714\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 8s 841ms/step - loss: 1.3095 - acc: 0.6043 - val_loss: 1.8138 - val_acc: 0.5724\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 9s 847ms/step - loss: 1.2953 - acc: 0.6081 - val_loss: 1.8023 - val_acc: 0.5781\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 8s 849ms/step - loss: 1.2833 - acc: 0.6111 - val_loss: 1.7861 - val_acc: 0.5781\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 9s 895ms/step - loss: 1.2663 - acc: 0.6173 - val_loss: 1.7695 - val_acc: 0.5848\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 9s 953ms/step - loss: 1.2574 - acc: 0.6195 - val_loss: 1.7567 - val_acc: 0.5857\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 9s 916ms/step - loss: 1.2401 - acc: 0.6264 - val_loss: 1.7427 - val_acc: 0.5914\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 1.2301 - acc: 0.6271 - val_loss: 1.7255 - val_acc: 0.5895\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 1.2137 - acc: 0.6303 - val_loss: 1.7233 - val_acc: 0.5952\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 1.2021 - acc: 0.6344 - val_loss: 1.6934 - val_acc: 0.6010\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 10s 972ms/step - loss: 1.1886 - acc: 0.6398 - val_loss: 1.6866 - val_acc: 0.6105\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 9s 929ms/step - loss: 1.1754 - acc: 0.6420 - val_loss: 1.6669 - val_acc: 0.6162\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 10s 974ms/step - loss: 1.1606 - acc: 0.6500 - val_loss: 1.6594 - val_acc: 0.6190\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 8s 764ms/step - loss: 1.1481 - acc: 0.6516 - val_loss: 1.6405 - val_acc: 0.6257\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 8s 793ms/step - loss: 1.1338 - acc: 0.6572 - val_loss: 1.6284 - val_acc: 0.6248\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 9s 942ms/step - loss: 1.1230 - acc: 0.6586 - val_loss: 1.6130 - val_acc: 0.6314\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 10s 949ms/step - loss: 1.1096 - acc: 0.6626 - val_loss: 1.6018 - val_acc: 0.6333\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 10s 968ms/step - loss: 1.0989 - acc: 0.6643 - val_loss: 1.5838 - val_acc: 0.6362\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 9s 902ms/step - loss: 1.0859 - acc: 0.6686 - val_loss: 1.5787 - val_acc: 0.6352\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 9s 945ms/step - loss: 1.0748 - acc: 0.6725 - val_loss: 1.5581 - val_acc: 0.6429\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 9s 914ms/step - loss: 1.0629 - acc: 0.6757 - val_loss: 1.5493 - val_acc: 0.6448\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 1.0521 - acc: 0.6800 - val_loss: 1.5309 - val_acc: 0.6571\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 9s 878ms/step - loss: 1.0403 - acc: 0.6848 - val_loss: 1.5263 - val_acc: 0.6533\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 10s 956ms/step - loss: 1.0287 - acc: 0.6869 - val_loss: 1.5054 - val_acc: 0.6676\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 9s 841ms/step - loss: 1.0180 - acc: 0.6936 - val_loss: 1.4970 - val_acc: 0.6619\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 10s 905ms/step - loss: 1.0094 - acc: 0.6965 - val_loss: 1.4791 - val_acc: 0.6705\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 7s 745ms/step - loss: 0.9945 - acc: 0.7011 - val_loss: 1.4676 - val_acc: 0.6733\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 7s 752ms/step - loss: 0.9883 - acc: 0.7015 - val_loss: 1.4591 - val_acc: 0.6810\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 10s 1s/step - loss: 0.9734 - acc: 0.7112 - val_loss: 1.4351 - val_acc: 0.6905\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 8s 835ms/step - loss: 0.9663 - acc: 0.7138 - val_loss: 1.4386 - val_acc: 0.6829\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 9s 900ms/step - loss: 0.9539 - acc: 0.7179 - val_loss: 1.4158 - val_acc: 0.6943\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 7s 754ms/step - loss: 0.9433 - acc: 0.7234 - val_loss: 1.4074 - val_acc: 0.6990\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 8s 772ms/step - loss: 0.9362 - acc: 0.7253 - val_loss: 1.3884 - val_acc: 0.7076\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 7s 721ms/step - loss: 0.9229 - acc: 0.7328 - val_loss: 1.3834 - val_acc: 0.7038\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 7s 680ms/step - loss: 0.9152 - acc: 0.7323 - val_loss: 1.3630 - val_acc: 0.7105\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 7s 747ms/step - loss: 0.9042 - acc: 0.7364 - val_loss: 1.3578 - val_acc: 0.7067\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 7s 719ms/step - loss: 0.8942 - acc: 0.7386 - val_loss: 1.3435 - val_acc: 0.7143\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 7s 757ms/step - loss: 0.8863 - acc: 0.7416 - val_loss: 1.3309 - val_acc: 0.7181\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 9s 775ms/step - loss: 0.8757 - acc: 0.7452 - val_loss: 1.3215 - val_acc: 0.7238\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 8s 760ms/step - loss: 0.8668 - acc: 0.7477 - val_loss: 1.3068 - val_acc: 0.7238\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.8568 - acc: 0.7503 - val_loss: 1.3001 - val_acc: 0.7257\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 9s 951ms/step - loss: 0.8489 - acc: 0.7530 - val_loss: 1.2836 - val_acc: 0.7362\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 12s 1s/step - loss: 0.8377 - acc: 0.7590 - val_loss: 1.2806 - val_acc: 0.7362\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 9s 928ms/step - loss: 0.8286 - acc: 0.7630 - val_loss: 1.2609 - val_acc: 0.7390\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 10s 985ms/step - loss: 0.8202 - acc: 0.7637 - val_loss: 1.2556 - val_acc: 0.7419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229a05945b0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "            batch_size = batch_size,\n",
    "            steps_per_epoch = train_samples//batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "            validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always remember to save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('Weights_ASL/nmt_weights.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights, if you close the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Weights_ASL/nmt_weights.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate the output sequence by invoking the above setup in a loop as follows\n",
    "\n",
    "Decode sample sequeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Input ASL sentence: i like books\n",
      "Actual English Translation:  i like books \n",
      "Predicted English Translation:  i like books \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input ASL sentence:', X_train.iloc[k:k+1].values[0])\n",
    "print('Actual English Translation:', y_train.iloc[k:k+1].values[0][6:-4])\n",
    "print('Predicted English Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Input ASL sentence: you like coffee tea which\n",
      "Actual English Translation:  which do you like coffee or tea \n",
      "Predicted English Translation:  which do you like to go to go \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input ASL sentence:', X_train.iloc[k:k+1].values[0])\n",
    "print('Actual English Translation:', y_train.iloc[k:k+1].values[0][6:-4])\n",
    "print('Predicted English Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Input ASL sentence: i hate trees\n",
      "Actual English Translation:  i hate trees \n",
      "Predicted English Translation:  i hate milk \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input ASL sentence:', X_train.iloc[k:k+1].values[0])\n",
    "print('Actual English Translation:', y_train.iloc[k:k+1].values[0][6:-4])\n",
    "print('Predicted English Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "asl_sentence = []\n",
    "true_eng_trans = []\n",
    "pred_eng_trans = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    k+=1\n",
    "    (input_seq, actual_output), _ = next(train_gen)\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    asl_sentence.append(X_train.iloc[k:k+1].values[0])\n",
    "    true_eng_trans.append(y_train.iloc[k:k+1].values[0][6:-4])\n",
    "    pred_eng_trans.append(decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ASL sentence: my kids i teach help help other people why develops good character\n",
      "Actual English Translation:  teaching my kids to help other people builds good character \n",
      "Predicted English Translation:  do you like bicycles \n",
      "\n",
      "Input ASL sentence: poem rhyme write can you\n",
      "Actual English Translation:  can you write a poem that rhymes \n",
      "Predicted English Translation:  the wrestling is class is zero zero zero and basket\n",
      "\n",
      "Input ASL sentence: yes i deaf\n",
      "Actual English Translation:  yes im deaf \n",
      "Predicted English Translation:  i was a in \n",
      "\n",
      "Input ASL sentence: most famous deaf pilots who oneofthree cal rodgers twoofthree nellie zabel willhite threeofthree rhulin thomas\n",
      "Actual English Translation:  three of the most famous deaf pilots are cal rodgers nellie zabel willhite and rhulin thomas \n",
      "Predicted English Translation:  hello brad nice to a get a slavery \n",
      "\n",
      "Input ASL sentence: butterfly color what purple\n",
      "Actual English Translation:  the butterfly is purple \n",
      "Predicted English Translation:  my sister was born in \n",
      "\n",
      "Input ASL sentence: cook dinner for you i want\n",
      "Actual English Translation:  i want to cook a dinner for you \n",
      "Predicted English Translation:  i like milk \n",
      "\n",
      "Input ASL sentence: black friday shopping i hate\n",
      "Actual English Translation:  i hate shopping on black friday \n",
      "Predicted English Translation:  how are you \n",
      "\n",
      "Input ASL sentence: time three zero zero pm football game\n",
      "Actual English Translation:  the football game is at three zero zero pm \n",
      "Predicted English Translation:  i have fifteen \n",
      "\n",
      "Input ASL sentence: my mother married where utah\n",
      "Actual English Translation:  my mother was married in utah \n",
      "Predicted English Translation:  the will will will at the zero zero zero zero and basket\n",
      "\n",
      "Input ASL sentence: i hate ice cream\n",
      "Actual English Translation:  i hate ice cream \n",
      "Predicted English Translation:  i save a was the of the in the in the fall \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Input ASL sentence:', asl_sentence[i])\n",
    "    print('Actual English Translation:', true_eng_trans[i])\n",
    "    print('Predicted English Translation:', pred_eng_trans[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17069188d88b1100ef242ce351d44cf343932479c7edb3acc83f5a4d63d17234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
