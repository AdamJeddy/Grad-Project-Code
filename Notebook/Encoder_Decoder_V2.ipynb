{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>ASL Gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you want a ride to the mall?</td>\n",
       "      <td>M-A-L-L RIDE WANT YOU Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes I want to start buying Christmas gifts soon</td>\n",
       "      <td>YES SOON CHRISTMAS GIFTS START BUYING WANT ME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please sit in this chair</td>\n",
       "      <td>THIS CHAIR PLEASE SIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like to fly small planes</td>\n",
       "      <td>SMALL PLANES FLY LIKE ME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He will go later</td>\n",
       "      <td>HE GO WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           English  \\\n",
       "0                  Do you want a ride to the mall?   \n",
       "1  Yes I want to start buying Christmas gifts soon   \n",
       "2                         Please sit in this chair   \n",
       "3                       I like to fly small planes   \n",
       "4                                 He will go later   \n",
       "\n",
       "                                       ASL Gloss  \n",
       "0                        M-A-L-L RIDE WANT YOU Q  \n",
       "1  YES SOON CHRISTMAS GIFTS START BUYING WANT ME  \n",
       "2                          THIS CHAIR PLEASE SIT  \n",
       "3                       SMALL PLANES FLY LIKE ME  \n",
       "4                                     HE GO WILL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ASL_English.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1670, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Note:\n",
    "- Replace the numbers/digits\n",
    "- Check regarding Finger spellings\n",
    "- Check if it is required to add start and end tokens to target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {'1': \"one \", '2':\"two \", '3':\"three \", '4':\"four \", '5':\"five \", '6':\"six \", '7':\"seven \", '8':\"eight \", '9':\"nine \", '0':\"zero \"}\n",
    "df['English'] = df['English'].apply(lambda x: re.sub('(\\d)', lambda m: replacements[m.group()], x))\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: re.sub('(\\d)', lambda m: replacements[m.group()], x))\n",
    "\n",
    "# Remove extra spaces\n",
    "df['English'] = df['English'].apply(lambda x: x.strip())\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: x.strip())\n",
    "\n",
    "# Lowercase all characters\n",
    "df['English'] = df['English'].apply(lambda x: x.lower())\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply (lambda x: x.lower())\n",
    "\n",
    "# Removing double spaces\n",
    "df['English'] = df['English'].apply(lambda x: x.replace('  ', ' '))\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: x.replace('  ', ' '))\n",
    "\n",
    "# Remove quotes # Might not need this\n",
    "# df['English'] = df['English'].apply(lambda x: re.sub (r\"'\", '', x))\n",
    "# df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: re.sub (r\"'\", '', x))\n",
    "\n",
    "# Remove all special character\n",
    "df['English'] = df['English'].apply(lambda x: ''.join (ch for ch in x if ch not in set(string.punctuation)))\n",
    "df['ASL Gloss'] = df['ASL Gloss'].apply(lambda x: ''.join (ch for ch in x if ch not in set(string.punctuation)))\n",
    "\n",
    "# Check if dataset has numbers\n",
    "#print(df['English'].str.contains(r'\\d').any())\n",
    "#print(df['ASL Gloss'].str.contains(r'\\d').any())\n",
    "\n",
    "# Add tokens to target sequence\n",
    "df['English'] = df['English'].apply(lambda x : 'START_ ' + x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>ASL Gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>START_ do you want a ride to the mall _END</td>\n",
       "      <td>mall ride want you q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>START_ yes i want to start buying christmas gi...</td>\n",
       "      <td>yes soon christmas gifts start buying want me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>START_ please sit in this chair _END</td>\n",
       "      <td>this chair please sit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>START_ i like to fly small planes _END</td>\n",
       "      <td>small planes fly like me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>START_ he will go later _END</td>\n",
       "      <td>he go will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>START_ mike is walking over to my house _END</td>\n",
       "      <td>my house mike walk will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>START_ i havent eaten _END</td>\n",
       "      <td>me eat not yetstructuring sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>START_ he ran _END</td>\n",
       "      <td>he ran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>START_ he ran _END</td>\n",
       "      <td>ran him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>START_ she fell _END</td>\n",
       "      <td>she fell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0         START_ do you want a ride to the mall _END   \n",
       "1  START_ yes i want to start buying christmas gi...   \n",
       "2               START_ please sit in this chair _END   \n",
       "3             START_ i like to fly small planes _END   \n",
       "4                       START_ he will go later _END   \n",
       "5       START_ mike is walking over to my house _END   \n",
       "6                         START_ i havent eaten _END   \n",
       "7                                 START_ he ran _END   \n",
       "8                                 START_ he ran _END   \n",
       "9                               START_ she fell _END   \n",
       "\n",
       "                                       ASL Gloss  \n",
       "0                           mall ride want you q  \n",
       "1  yes soon christmas gifts start buying want me  \n",
       "2                          this chair please sit  \n",
       "3                       small planes fly like me  \n",
       "4                                     he go will  \n",
       "5                        my house mike walk will  \n",
       "6            me eat not yetstructuring sentences  \n",
       "7                                         he ran  \n",
       "8                                        ran him  \n",
       "9                                       she fell  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ASL Gloss'].str.len().sort_values(ascending=False).head()\n",
    "# df['English'].str.len().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_eng_words = set()\n",
    "for eng in df['English']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of ASL \n",
    "all_ASL_words = set()\n",
    "for asl in df['ASL Gloss']:\n",
    "    for word in asl.split():\n",
    "        if word not in all_ASL_words:\n",
    "            all_ASL_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length target:  25\n"
     ]
    }
   ],
   "source": [
    "# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in df ['English']:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "print(\"Max length target: \", max_length_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length sorce:  18\n"
     ]
    }
   ],
   "source": [
    "# Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in df ['ASL Gloss']:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "print(\"Max length sorce: \", max_length_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1948, 2002)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_ASL_words))\n",
    "target_words = sorted(list(all_eng_words))\n",
    "\n",
    "# Calculate Vocab size for both source and target\n",
    "num_encoder_tokens = len(all_ASL_words) + 1\n",
    "num_decoder_tokens = len(all_eng_words) + 1\n",
    "\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word to token dictionary for both source and target\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "# Create token to word dictionary for both source and target\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>ASL Gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>START_ labor day is monday september one st _END</td>\n",
       "      <td>labor day when monday september one st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>START_ i like trees _END</td>\n",
       "      <td>i like trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>START_ today the weather is snowy _END</td>\n",
       "      <td>weather today what snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>START_ bill was born in one nine six four  _END</td>\n",
       "      <td>year one nine six four bill born</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>START_ emily was born in the month of march _END</td>\n",
       "      <td>emily born month march</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>START_ there is no way im buying that bag it c...</td>\n",
       "      <td>that bag no i not buy cost an arm and a leg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>START_ hanukkah is sunday december two one  _END</td>\n",
       "      <td>hanukkah when sunday december two one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>START_ my aunt was married in alabama _END</td>\n",
       "      <td>my aunt married where alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>START_ my aunt lives in maine _END</td>\n",
       "      <td>my aunt live where maine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>START_ do you love playing the piano _END</td>\n",
       "      <td>you love piano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                English  \\\n",
       "1387   START_ labor day is monday september one st _END   \n",
       "986                            START_ i like trees _END   \n",
       "1423             START_ today the weather is snowy _END   \n",
       "1156    START_ bill was born in one nine six four  _END   \n",
       "1168   START_ emily was born in the month of march _END   \n",
       "964   START_ there is no way im buying that bag it c...   \n",
       "1391   START_ hanukkah is sunday december two one  _END   \n",
       "1597         START_ my aunt was married in alabama _END   \n",
       "1617                 START_ my aunt lives in maine _END   \n",
       "1087          START_ do you love playing the piano _END   \n",
       "\n",
       "                                        ASL Gloss  \n",
       "1387       labor day when monday september one st  \n",
       "986                                  i like trees  \n",
       "1423                      weather today what snow  \n",
       "1156             year one nine six four bill born  \n",
       "1168                       emily born month march  \n",
       "964   that bag no i not buy cost an arm and a leg  \n",
       "1391       hanukkah when sunday december two one   \n",
       "1597                my aunt married where alabama  \n",
       "1617                     my aunt live where maine  \n",
       "1087                               you love piano  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(df).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a 90–10 train and test split and write a Python generator function to load the data in batches as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1503,), (167,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train - Test Split\n",
    "X, y = df['ASL Gloss'], df['English']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the train and test dataframes for reproducing the results later, as they are shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('Weights_ASL/X_train.pkl')\n",
    "X_test.to_pickle('Weights_ASL/X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X.iloc[j:j+batch_size], y.iloc[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder - Decoder Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# Use a softmax to generate a probability distribution over the target vocabulary for each time step\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network for 50 epochs with a batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "# epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 24s 1s/step - loss: 7.5888 - acc: 0.1077 - val_loss: 7.5743 - val_acc: 0.1338\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 4s 336ms/step - loss: 7.5543 - acc: 0.1316 - val_loss: 7.5255 - val_acc: 0.1338\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 3s 320ms/step - loss: 7.4333 - acc: 0.1322 - val_loss: 7.2704 - val_acc: 0.1338\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 4s 338ms/step - loss: 6.9586 - acc: 0.1316 - val_loss: 6.6637 - val_acc: 0.1338\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 4s 313ms/step - loss: 6.2809 - acc: 0.1322 - val_loss: 6.0776 - val_acc: 0.1338\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 3s 314ms/step - loss: 5.7294 - acc: 0.1316 - val_loss: 5.7594 - val_acc: 0.1338\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 4s 359ms/step - loss: 5.4772 - acc: 0.1322 - val_loss: 5.5804 - val_acc: 0.1338\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 5.2872 - acc: 0.1316 - val_loss: 5.5138 - val_acc: 0.1338\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 4s 351ms/step - loss: 5.2503 - acc: 0.1322 - val_loss: 5.4403 - val_acc: 0.1338\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 4s 315ms/step - loss: 5.1477 - acc: 0.1316 - val_loss: 5.4217 - val_acc: 0.1338\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 3s 290ms/step - loss: 5.1573 - acc: 0.1322 - val_loss: 5.3678 - val_acc: 0.1338\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 3s 290ms/step - loss: 5.0762 - acc: 0.1316 - val_loss: 5.3633 - val_acc: 0.1338\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 3s 301ms/step - loss: 5.1011 - acc: 0.1322 - val_loss: 5.3175 - val_acc: 0.1338\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 3s 322ms/step - loss: 5.0294 - acc: 0.1316 - val_loss: 5.3201 - val_acc: 0.1338\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 4s 318ms/step - loss: 5.0609 - acc: 0.1322 - val_loss: 5.2791 - val_acc: 0.1338\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 4s 405ms/step - loss: 4.9942 - acc: 0.1316 - val_loss: 5.2860 - val_acc: 0.1338\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 4s 359ms/step - loss: 5.0292 - acc: 0.1322 - val_loss: 5.2488 - val_acc: 0.1338\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 4s 339ms/step - loss: 4.9654 - acc: 0.1316 - val_loss: 5.2588 - val_acc: 0.1338\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 4s 343ms/step - loss: 5.0028 - acc: 0.1322 - val_loss: 5.2235 - val_acc: 0.1338\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 3s 313ms/step - loss: 4.9405 - acc: 0.1316 - val_loss: 5.2346 - val_acc: 0.1338\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 3s 311ms/step - loss: 4.9783 - acc: 0.1322 - val_loss: 5.1994 - val_acc: 0.1338\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 4.9147 - acc: 0.1316 - val_loss: 5.2094 - val_acc: 0.1358\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 4s 316ms/step - loss: 4.9519 - acc: 0.1343 - val_loss: 5.1748 - val_acc: 0.1358\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 3s 271ms/step - loss: 4.8885 - acc: 0.1398 - val_loss: 5.1862 - val_acc: 0.1473\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 3s 304ms/step - loss: 4.9266 - acc: 0.1456 - val_loss: 5.1523 - val_acc: 0.1452\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 3s 306ms/step - loss: 4.8633 - acc: 0.1463 - val_loss: 5.1646 - val_acc: 0.1494\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 3s 285ms/step - loss: 4.9026 - acc: 0.1464 - val_loss: 5.1314 - val_acc: 0.1484\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 3s 303ms/step - loss: 4.8396 - acc: 0.1476 - val_loss: 5.1440 - val_acc: 0.1494\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 3s 279ms/step - loss: 4.8802 - acc: 0.1481 - val_loss: 5.1114 - val_acc: 0.1473\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 3s 306ms/step - loss: 4.8174 - acc: 0.1485 - val_loss: 5.1246 - val_acc: 0.1484\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 3s 313ms/step - loss: 4.8590 - acc: 0.1500 - val_loss: 5.0922 - val_acc: 0.1484\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 4.7963 - acc: 0.1512 - val_loss: 5.1056 - val_acc: 0.1494\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 4s 339ms/step - loss: 4.8386 - acc: 0.1525 - val_loss: 5.0734 - val_acc: 0.1755\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 4.7759 - acc: 0.1563 - val_loss: 5.0871 - val_acc: 0.1505\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 4s 411ms/step - loss: 4.8187 - acc: 0.1562 - val_loss: 5.0550 - val_acc: 0.1839\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 4s 346ms/step - loss: 4.7560 - acc: 0.1595 - val_loss: 5.0687 - val_acc: 0.1557\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 4.7992 - acc: 0.1608 - val_loss: 5.0365 - val_acc: 0.1839\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 3s 305ms/step - loss: 4.7361 - acc: 0.1660 - val_loss: 5.0502 - val_acc: 0.1599\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 3s 311ms/step - loss: 4.7794 - acc: 0.1689 - val_loss: 5.0176 - val_acc: 0.1860\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 4.7158 - acc: 0.1847 - val_loss: 5.0306 - val_acc: 0.1860\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 4.7587 - acc: 0.1888 - val_loss: 4.9973 - val_acc: 0.1850\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 4s 351ms/step - loss: 4.6942 - acc: 0.1907 - val_loss: 5.0098 - val_acc: 0.1860\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 4.7364 - acc: 0.1890 - val_loss: 4.9755 - val_acc: 0.1839\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 4.6708 - acc: 0.1922 - val_loss: 4.9867 - val_acc: 0.1870\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 3s 281ms/step - loss: 4.7123 - acc: 0.1921 - val_loss: 4.9511 - val_acc: 0.1850\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 4.6454 - acc: 0.1939 - val_loss: 4.9612 - val_acc: 0.1881\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 3s 277ms/step - loss: 4.6854 - acc: 0.1928 - val_loss: 4.9238 - val_acc: 0.1881\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 3s 306ms/step - loss: 4.6170 - acc: 0.1942 - val_loss: 4.9323 - val_acc: 0.1860\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 3s 266ms/step - loss: 4.6557 - acc: 0.1899 - val_loss: 4.8938 - val_acc: 0.1902\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 4.5873 - acc: 0.1917 - val_loss: 4.9021 - val_acc: 0.1839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c7d5829150>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "            batch_size = batch_size,\n",
    "            steps_per_epoch = train_samples//batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "            validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always remember to save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('Weights_ASL/nmt_weights.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights, if you close the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Weights_ASL/nmt_weights.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate the output sequence by invoking the above setup in a loop as follows\n",
    "\n",
    "Decode sample sequeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Input ASL sentence: my favorite kind popcorn which kettle corn\n",
      "Actual English Translation:  my favorite kind of popcorn is kettle corn \n",
      "Predicted English Translation:  my my is is \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input ASL sentence:', X_train.iloc[k:k+1].values[0])\n",
    "print('Actual English Translation:', y_train.iloc[k:k+1].values[0][6:-4])\n",
    "print('Predicted English Translation:', decoded_sentence[:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17069188d88b1100ef242ce351d44cf343932479c7edb3acc83f5a4d63d17234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
